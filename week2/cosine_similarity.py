# -*- coding: utf-8 -*-
"""cosine_similarity.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13LqNYzaFj3c_g88j3scJW80d5qy8oKFQ
"""

! pip install -q transformers torch

from transformers import AutoTokenizer, AutoModel


model_name = 'sentence-transformers/all-MiniLM-L6-v2'
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

from sklearn.metrics.pairwise import cosine_similarity
import torch

def get_embedding(sentence):
    inputs = tokenizer(sentence, return_tensors='pt', padding=True, truncation=True)
    with torch.no_grad():
        outputs = model(**inputs)
        # Use the mean of the last hidden state as sentence embedding
        embeddings = outputs.last_hidden_state.mean(dim=1)
    return embeddings

sentence1 = "explain about cosine similarity"
sentence2 = "Cosine similarity is a metric used to measure how similar two vectors are — regardless of their magnitude — by calculating the cosine of the angle between them."


embedding1 = get_embedding(sentence1)
embedding2 = get_embedding(sentence2)

similarity = cosine_similarity(embedding1.numpy(), embedding2.numpy())
print(f"Cosine similarity: {similarity[0][0]:.4f}")





